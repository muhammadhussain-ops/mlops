# Exam template for 02476 Machine Learning Operations

This is the report template for the exam. Please only remove the text formatted as with three dashes in front and behind
like:

```--- question 1 fill here ---```

Where you instead should add your answers. Any other changes may have unwanted consequences when your report is
auto-generated at the end of the course. For questions where you are asked to include images, start by adding the image
to the `figures` subfolder (please only use `.png`, `.jpg` or `.jpeg`) and then add the following code in your answer:

```markdown
![my_image](figures/<image>.<extension>)
```

In addition to this markdown file, we also provide the `report.py` script that provides two utility functions:

Running:

```bash
python report.py html
```

Will generate a `.html` page of your report. After the deadline for answering this template, we will auto-scrape
everything in this `reports` folder and then use this utility to generate a `.html` page that will be your serve
as your final hand-in.

Running

```bash
python report.py check
```

Will check your answers in this template against the constraints listed for each question e.g. is your answer too
short, too long, or have you included an image when asked. For both functions to work you mustn't rename anything.
The script has two dependencies that can be installed with

```bash
pip install typer markdown
```

## Overall project checklist

The checklist is *exhaustive* which means that it includes everything that you could do on the project included in the
curriculum in this course. Therefore, we do not expect at all that you have checked all boxes at the end of the project.
The parenthesis at the end indicates what module the bullet point is related to. Please be honest in your answers, we
will check the repositories and the code to verify your answers.

### Week 1

* [x] Create a git repository (M5)
* [x] Make sure that all team members have write access to the GitHub repository (M5)
* [x] Create a dedicated environment for you project to keep track of your packages (M2)
* [x] Create the initial file structure using cookiecutter with an appropriate template (M6)
* [x] Fill out the `data.py` file such that it downloads whatever data you need and preprocesses it (if necessary) (M6)
* [x] Add a model to `model.py` and a training procedure to `train.py` and get that running (M6)
* [x] Remember to fill out the `requirements.txt` and `requirements_dev.txt` file with whatever dependencies that you
    are using (M2+M6)
* [ ] Remember to comply with good coding practices (`pep8`) while doing the project (M7)
* [ ] Do a bit of code typing and remember to document essential parts of your code (M7)
* [x] Setup version control for your data or part of your data (M8)
* [x] Add command line interfaces and project commands to your code where it makes sense (M9)
* [x] Construct one or multiple docker files for your code (M10)
* [x] Build the docker files locally and make sure they work as intended (M10)
* [x] Write one or multiple configurations files for your experiments (M11)
* [ ] Used Hydra to load the configurations and manage your hyperparameters (M11)
* [ ] Use profiling to optimize your code (M12)
* [x] Use logging to log important events in your code (M14)
* [ ] Use Weights & Biases to log training progress and other important metrics/artifacts in your code (M14)
* [ ] Consider running a hyperparameter optimization sweep (M14)
* [ ] Use PyTorch-lightning (if applicable) to reduce the amount of boilerplate in your code (M15)

### Week 2

* [x] Write unit tests related to the data part of your code (M16)
* [x] Write unit tests related to model construction and or model training (M16)
* [ ] Calculate the code coverage (M16)
* [ ] Get some continuous integration running on the GitHub repository (M17)
* [ ] Add caching and multi-os/python/pytorch testing to your continuous integration (M17)
* [ ] Add a linting step to your continuous integration (M17)
* [ ] Add pre-commit hooks to your version control setup (M18)
* [ ] Add a continues workflow that triggers when data changes (M19)
* [ ] Add a continues workflow that triggers when changes to the model registry is made (M19)
* [x] Create a data storage in GCP Bucket for your data and link this with your data version control setup (M21)
* [ ] Create a trigger workflow for automatically building your docker images (M21)
* [x] Get your model training in GCP using either the Engine or Vertex AI (M21)
* [x] Create a FastAPI application that can do inference using your model (M22)
* [x] Deploy your model in GCP using either Functions or Run as the backend (M23)
* [ ] Write API tests for your application and setup continues integration for these (M24)
* [ ] Load test your application (M24)
* [ ] Create a more specialized ML-deployment API using either ONNX or BentoML, or both (M25)
* [ ] Create a frontend for your API (M26)

### Week 3

* [ ] Check how robust your model is towards data drifting (M27)
* [ ] Deploy to the cloud a drift detection API (M27)
* [ ] Instrument your API with a couple of system metrics (M28)
* [ ] Setup cloud monitoring of your instrumented application (M28)
* [ ] Create one or more alert systems in GCP to alert you if your app is not behaving correctly (M28)
* [ ] If applicable, optimize the performance of your data loading using distributed data loading (M29)
* [ ] If applicable, optimize the performance of your training pipeline by using distributed training (M30)
* [ ] Play around with quantization, compilation and pruning for you trained models to increase inference speed (M31)

### Extra

* [ ] Write some documentation for your application (M32)
* [ ] Publish the documentation to GitHub Pages (M32)
* [ ] Revisit your initial project description. Did the project turn out as you wanted?
* [ ] Create an architectural diagram over your MLOps pipeline
* [ ] Make sure all group members have an understanding about all parts of the project
* [x] Uploaded all your code to GitHub

## Group information

### Question 1
> **Enter the group number you signed up on <learn.inside.dtu.dk>**
>
> Answer:

Group number:  94

### Question 2
> **Enter the study number for each member in the group**
>
> Example:
>
> *sXXXXXX, sXXXXXX, sXXXXXX*
>
> Answer:

s225785, s224229, s224176

### Question 3
> **A requirement to the project is that you include a third-party package not covered in the course. What framework**
> **did you choose to work with and did it help you complete the project?**
>
> Recommended answer length: 100-200 words.
>
> Example:
> *We used the third-party framework ... in our project. We used functionality ... and functionality ... from the*
> *package to do ... and ... in our project*.
>
> Answer:

TQDM:
We used the package tqdm which visualizes the number of iterations completed in a loop. This user-friendly tool is highly beneficial when executing computationally intensive or data-heavy tasks.

PIL: 
To import and export images in different formats we used the library Pillow/PIL. This library provided a vast range of functionalities when working with images, such as processing images of different file format.

ByteIO:
The functionality of this package is a bit more technical, then that of the other packages. The BytesIO object allowed the Image.open() function to read the data as if it were a file, even though it’s actually just raw bytes in memory.


## Coding environment

> In the following section we are interested in learning more about you local development environment. This includes
> how you managed dependencies, the structure of your code and how you managed code quality.

### Question 4

> **Explain how you managed dependencies in your project? Explain the process a new team member would have to go**
> **through to get an exact copy of your environment.**
>
> Recommended answer length: 100-200 words
>
> Example:
> *We used ... for managing our dependencies. The list of dependencies was auto-generated using ... . To get a*
> *complete copy of our development environment, one would have to run the following commands*
>
> Answer:

To manage our dependancies we created a requirements.txt file as advised in the module M2.
 This txt-file contained all relevent packages to execute our project.
  Through the use of dependabot, we made sure to that all of our packages were up to data on the GitHub workflow. 

To get a complete copy of our development environment one would have to run the following commands: 

“python -m pip install -U pip setuptools wheel
          pip install -r requirements.txt
          pip list
          pip install -e .
”


### Question 5

> **We expect that you initialized your project using the cookiecutter template. Explain the overall structure of your**
> **code. What did you fill out? Did you deviate from the template in some way?**
>
> Recommended answer length: 100-200 words
>
> Example:
> *From the cookiecutter template we have filled out the ... , ... and ... folder. We have removed the ... folder*
> *because we did not use any ... in our project. We have added an ... folder that contains ... for running our*
> *experiments.*
>
> Answer:

We initialized our project using the Cookiecutter template, which provided a standardized and organized foundation. 
However, we customized the structure to better fit our needs by integrating Google Cloud Storage for all our files and code, 
eliminating the need for local storage. Consequently, we only utilized specific files from the template that were relevant 
to our cloud-based approach, such as configuration files for cloud integration and essential scripts for deployment.
These deviations allowed us to leverage cloud resources effectively, ensuring our project remains scalable and maintainable. 
By adapting the Cookiecutter template to our requirements, we achieved a robust and tailored codebase that supports our 
specific workflow and deployment strategies. The use of cookiecutter also resulted in a better and more userfriendly environment
for us to build the api method in the project.

### Question 6

> **Did you implement any rules for code quality and format? What about typing and documentation? Additionally,**
> **explain with your own words why these concepts matters in larger projects.**
>
> Recommended answer length: 100-200 words.
>
> Example:
> *We used ... for linting and ... for formatting. We also used ... for typing and ... for documentation. These*
> *concepts are important in larger projects because ... . For example, typing ...*
>
> Answer:

We used some of the techniques taught in section 2 to format the code and files for user-friendliness. 
This is very important for big projects since it will be chaotic to find specific files and functions in big projects if 
the project structure isn't formatted in a structural way. For example, as in the file structure provided by the course material, 
it is clear to see that everything is formatted and distributed such that it is easy to find the desired code 
or file in a big project.

## Version control

> In the following section we are interested in how version control was used in your project during development to
> corporate and increase the quality of your code.

### Question 7

> **How many tests did you implement and what are they testing in your code?**
>
> Recommended answer length: 50-100 words.
>
> Example:
> *In total we have implemented X tests. Primarily we are testing ... and ... as these the most critical parts of our*
> *application but also ... .*
>
> Answer:

In this part of the project we maybe tried to be a bit too ambitious but integrating google-cloud in all off our tests,
 instead of starting of by executing the tests locally. The unfortunately lead to the tests not being fully developed,
  but we will try sharing our thoughts and ideas in this section. 

First we wanted to evaluate the data, by testing the dimensions and number of images loaded of our data.
Second we wanted to evaluate the model, by evaluating the accuracy of it and making sure the weights were not equal to zero.

### Question 8

> **What is the total code coverage (in percentage) of your code? If your code had a code coverage of 100% (or close**
> **to), would you still trust it to be error free? Explain you reasoning.**
>
> Recommended answer length: 100-200 words.
>
> Example:
> *The total code coverage of code is X%, which includes all our source code. We are far from 100% coverage of our **
> *code and even if we were then...*
>
> Answer:

If the tests had been developed as expected, we would have a code coverage of around 90%. 
The estimate is based of the size of our workflow which isn’t very large, and covering most of the code would not be that difficult. 
Testing the connection to google cloud were not a part of the tests we wanted to develop, which makes the missing 10%.

However, even if we were at 100% coverage, we would not trust the code to be error-free. This is because code coverage only
measures the percentage of code that is executed by the tests, not the quality of the tests themselves.
Therefore, even with 100% coverage, there could still be bugs in the code that are not caught by the tests.


### Question 9

> **Did you workflow include using branches and pull requests? If yes, explain how. If not, explain how branches and**
> **pull request can help improve version control.**
>
> Recommended answer length: 100-200 words.
>
> Example:
> *We made use of both branches and PRs in our project. In our group, each member had an branch that they worked on in*
> *addition to the main branch. To merge code we ...*
>
> Answer:

We made use of branches when divding the workload between us. This helped us avoid merge conflicts most of the times.
Sometimes because of the lack of communication, two or more group members worked on the same branch which usually resulted in merge conflicts. 
It should also be said that these conflicts were solved relatively fast.

--- question 9 fill here ---

### Question 10

> **Did you use DVC for managing data in your project? If yes, then how did it improve your project to have version**
> **control of your data. If no, explain a case where it would be beneficial to have version control of your data.**
>
> Recommended answer length: 100-200 words.
>
> Example:
> *We did make use of DVC in the following way: ... . In the end it helped us in ... for controlling ... part of our*
> *pipeline*
>
> Answer:

--- question 10 fill here ---
We implemented DVC in the start of the project to store our data and for possible future version control. But this became 
irrelevant as we instead used Google Cloud Storage for all our data and code. Furthermore we did not have different versions 
of our data, as we only had one dataset that was used throughout the project. However, if we had multiple datasets or versions
of the same dataset, DVC could have been used.


### Question 11

> **Discuss you continuous integration setup. What kind of continuous integration are you running (unittesting,**
> **linting, etc.)? Do you test multiple operating systems, Python  version etc. Do you make use of caching? Feel free**
> **to insert a link to one of your GitHub actions workflow.**
>
> Recommended answer length: 200-300 words.
>
> Example:
> *We have organized our continuous integration into 3 separate files: one for doing ..., one for running ... testing*
> *and one for running ... . In particular for our ..., we used ... .An example of a triggered workflow can be seen*
> *here: <weblink>*
>
> Answer:

Another consequence of the lack of structure and overly ambitious goals in the project, was that we did not implement continuous integration.
We had planned to use GitHub actions to run the tests and linting on the code, but as referenced in the question on testing, we did not manage to implement it.
The plan was to have a workflow that would run the tests and linting on the code whenever a push was made to the main branch. Further work would have been 
to implement caching to speed up the workflow, but this was not implemented in the project.


--- question 11 fill here ---

## Running code and tracking experiments

> In the following section we are interested in learning more about the experimental setup for running your code and
> especially the reproducibility of your experiments.

### Question 12

> **How did you configure experiments? Did you make use of config files? Explain with coding examples of how you would**
> **run a experiment.**
>
> Recommended answer length: 50-100 words.
>
> Example:
> *We used a simple argparser, that worked in the following way: Python  my_script.py --lr 1e-3 --batch_size 25*
>
> Answer:

We used config files that were semi-hard coded. The config files were accessed by Omegaconf and used to set the hyperparameters for the model which 
we then would call on in the main config file when using vertex AI.

example: 

# hyp_config.yaml

hyperparameters:
 lr: x
 num_epochs: x
 batch_size: x
 small_subset: x # this parameter was for partitioning our dataset from cloud for testing purposes.

--- question 12 fill here ---

### Question 13

> **Reproducibility of experiments are important. Related to the last question, how did you secure that no information**
> **is lost when running experiments and that your experiments are reproducible?**
>
> Recommended answer length: 100-200 words.
>
> Example:
> *We made use of config files. Whenever an experiment is run the following happens: ... . To reproduce an experiment*
> *one would have to do ...*
>
> Answer:

We made use of config files to ensure that all hyperparameters and settings were stored and easily accessible. The config files for 
each experiment were saved in a gcs bucket with version control. This allowed us to easily reproduce experiments by simply
retrieving the config file from the bucket and running the experiment with the same settings. As each experiment was saved in the
bucket's "history", we could easily track the progress of each experiment and compare the results, along with the corresponding
hyperparameters used.

--- question 13 fill here ---

### Question 14

> **Upload 1 to 3 screenshots that show the experiments that you have done in W&B (or another experiment tracking**
> **service of your choice). This may include loss graphs, logged images, hyperparameter sweeps etc. You can take**
> **inspiration from [this figure](figures/wandb.png). Explain what metrics you are tracking and why they are**
> **important.**
>
> Recommended answer length: 200-300 words + 1 to 3 screenshots.
>
> Example:
> *As seen in the first image when have tracked ... and ... which both inform us about ... in our experiments.*
> *As seen in the second image we are also tracking ... and ...*
>
> Answer:

We were unfortunately not able to implement W&B in our project. To start with we used simple logging using print statements
and the tqdm module to track the progress of our experiments. The plan afterwards was to further implement W&B to track the
progress of our experiments. But due to issues we had with implementation and a lack of structured progress in the project,
we were not able to implement better logging.

--- question 14 fill here ---

### Question 15

> **Docker is an important tool for creating containerized applications. Explain how you used docker in your**
> **experiments/project? Include how you would run your docker images and include a link to one of your docker files.**
>
> Recommended answer length: 100-200 words.
>
> Example:
> *For our project we developed several images: one for training, inference and deployment. For example to run the*
> *training docker image: `docker run trainer:latest lr=1e-3 batch_size=64`. Link to docker file: <weblink>*
>
> Answer:

Docker was used to create a containerized environment for our project. This was done with the main intention of creating images,
which were then uploaded to the gcloud artifact registry for deployment using vertex AI. The docker images were created, tagged, and pushed using the following commands:

docker build -f dockerfiles/main.dockerfile -t gcr.io/halogen-country-447611-g0/mlops-vertex-image:latest .
docker tag gcr.io/halogen-country-447611-g0/mlops-vertex-image:latest europe-west1-docker.pkg.dev/halogen-country-447611-g0/ml-repo-224229/mlops-vertex-image:latest
docker push europe-west1-docker.pkg.dev/halogen-country-447611-g0/ml-repo-224229/mlops-vertex-image:latest

link to our main docker file:  mlops-bucket-224229-1/dockerfiles/main.dockerfile



--- question 15 fill here ---

### Question 16

> **When running into bugs while trying to run your experiments, how did you perform debugging? Additionally, did you**
> **try to profile your code or do you think it is already perfect?**
>
> Recommended answer length: 100-200 words.
>
> Example:
> *Debugging method was dependent on group member. Some just used ... and others used ... . We did a single profiling*
> *run of our main code at some point that showed ...*
>
> Answer:

--- question 16 fill here ---

Debugging was performed using a combination of logging print statements, and then the vs code debugger. The debugger was used to
step through the code and identify the source of the error. The print statements were used to track the progress of the code and
give us an idea of where to set the initial breakpoints. When this was insufficient, such as when the errors were related to authentication,
connection, or other non-locally contained issues, we used the logs from the vertex ai to identify the source of the error.
When all else failed the errors were copied into google search or chatGPT to get a better understanding of the error and how to fix it.
Profiling was nonexistent as the focus was on creating a final product before the deadline, and the code was not optimized for performance.

## Working in the cloud

> In the following section we would like to know more about your experience when developing in the cloud.

### Question 17

> **List all the GCP services that you made use of in your project and shortly explain what each service does?**
>
> Recommended answer length: 50-200 words.
aiplatform.googleapis.com            Vertex AI API (used for training and deploying models)
apigateway.googleapis.com            API Gateway API (used for creating, deploying, and managing APIs)
artifactregistry.googleapis.com      Artifact Registry API (used for storing and managing container images and secrets)
iam.googleapis.com                   Identity and Access Management (IAM) API (used for managing access control)
iamcredentials.googleapis.com        IAM Service Account Credentials API (used for creating and managing service account keys and permissions)
storage-api.googleapis.com           Google Cloud Storage JSON API (used for storing and managing data and as a place for the model to call files from)
storage.googleapis.com               Cloud Storage API (used for storing and managing data and files)

other services were of course used, for example BigQuery, cloudbuild, oslogin, source.googleapis etc... but the above-mentioned services
were the most widely implemented services in the project.


>
> Example:
> *We used the following two services: Engine and Bucket. Engine is used for... and Bucket is used for...*
>
> Answer:

--- question 17 fill here ---

### Question 18

> **The backbone of GCP is the Compute engine. Explained how you made use of this service and what type of VMs**
> **you used?**
>
> Recommended answer length: 100-200 words.
>
> Example:
> *We used the compute engine to run our ... . We used instances with the following hardware: ... and we started the*
> *using a custom container: ...*
>
> Answer:

We used the compute engine in combination with vertex AI to train our model. The compute engine was used to create a virtual machine
that was used to run the docker images that were created for the project. The VM was created with the following hardware: n1-standard-4
(4 vCPUs, 15 GB memory). The VM was started using a custom container that was created using the dockerfiles that were created for the project.
The VM was used to train the model and then the model was saved to the cloud storage bucket for deployment.

--- question 18 fill here ---

### Question 19

> **Insert 1-2 images of your GCP bucket, such that we can see what data you have stored in it.**
> **You can take inspiration from [this figure](figures/bucket.png).**
>
> Answer:

![SS of our GCP bucket](figures/q19.jpeg)

### Question 20

> **Upload 1-2 images of your GCP artifact registry, such that we can see the different docker images that you have**
> **stored. You can take inspiration from [this figure](figures/registry.jpeg).**
>
> Answer:

![SS of our GCP artifact registry](figures/q20.jpeg)


--- question 20 fill here ---

### Question 21

> **Upload 1-2 images of your GCP cloud build history, so we can see the history of the images that have been build in**
> **your project. You can take inspiration from [this figure](figures/build.png).**
>
> Answer:

A short explanation for the emptiness here, we used almost exclusively the local docker build and push to the gcloud artifact registry in order to run 
the images in the cloud. Otherwise the code was mainly run locally or through a local api for testing. The cloud build was not used as much as we had hoped.

![SS of our GCP cloud build history](figures/q21.jpeg)


--- question 21 fill here ---

### Question 22

> **Did you manage to train your model in the cloud using either the Engine or Vertex AI? If yes, explain how you did**
> **it. If not, describe why.**
>
> Recommended answer length: 100-200 words.
>
> Example:
> *We managed to train our model in the cloud using the Engine. We did this by ... . The reason we choose the Engine*
> *was because ...*
>
> Answer:

We were unable to train our model on the cloud within the project timeframe. However, we have established a cloud architecture 
that is nearly complete and requires only a few final adjustments to become fully operational. This setup lays the groundwork 
for future cloud-based training, which we intend to implement once the necessary refinements are made.

In the meantime, we have successfully deployed a local version of the model that can be trained using our API. 
This local implementation allows us to continue developing and testing the model training process without relying on cloud resources.
 The API-driven approach facilitates seamless interaction with the model, enabling efficient training workflows and easier 
 integration with other components of our project.

## Deployment

### Question 23

> **Did you manage to write an API for your model? If yes, explain how you did it and if you did anything special. If**
> **not, explain how you would do it.**
>
> Recommended answer length: 100-200 words.
>
> Example:
> *We did manage to write an API for our model. We used FastAPI to do this. We did this by ... . We also added ...*
> *to the API to make it more ...*
>
> Answer:

We have implemented API functionality for our program. We started by using Flask but switched to FastAPI since it was covered in
the course material. We integrated FastAPI into the codebase, allowing a server to be started within the project. 
When this server starts, it becomes possible to communicate with the project's main file. 
There are three available endpoints: train, evaluate, and inference.
The train function operates by connecting to the data folder and training the model by importing the neural network from 
the model folder. Once trained, the model is saved back into the model folder. The evaluate function retrieves the model and 
loads the weights from the model file, which were saved during the training process. It then evaluates the model using the entire
dataset. The inference function takes a single data point and feeds it into the model using the weights obtained from the training 
function.
With this setup, users of the project do not need direct access to the various files and folders, as all interactions are 
managed through the main file.

### Question 24

> **Did you manage to deploy your API, either in locally or cloud? If not, describe why. If yes, describe how and**
> **preferably how you invoke your deployed service?**
>
> Recommended answer length: 100-200 words.
>
> Example:
> *For deployment we wrapped our model into application using ... . We first tried locally serving the model, which*
> *worked. Afterwards we deployed it in the cloud, using ... . To invoke the service an user would call*
> *`curl -X POST -F "file=@file.json"<weburl>`*
>
> Answer:

Yes, we successfully deployed our API locally using FastAPI. Initially, we considered Flask, but FastAPI was selected to align 
with our course material. The local deployment allows users to interact with three endpoints.
To invoke the service an user would call: http://localhost:8000/train

### Question 25

> **Did you perform any unit testing and load testing of your API? If yes, explain how you did it and what results for**
> **the load testing did you get. If not, explain how you would do it.**
>
> Recommended answer length: 100-200 words.
>
> Example:
> *For unit testing we used ... and for load testing we used ... . The results of the load testing showed that ...*
> *before the service crashed.*
>

We did not implement load testing. However, if we had more time, we would have done it in the following way: 
We would use API testing to set up simulations of possible calls to our server. From there, using the 
Google Cloud Storage monitoring dashboard system, we would be able to see the load our project is subjected to. 
We were considering using the Locust framework for load testing.



### Question 26

> **Did you manage to implement monitoring of your deployed model? If yes, explain how it works. If not, explain how**
> **monitoring would help the longevity of your application.**
>
> Recommended answer length: 100-200 words.
>
> Example:
> *We did not manage to implement monitoring. We would like to have monitoring implemented such that over time we could*
> *measure ... and ... that would inform us about this ... behaviour of our application.*
>
> Answer:

No, we did not implement monitoring of our deployed model. However, implementing monitoring would significantly enhance the 
longevity of our application by allowing us to track the model's performance and health in real-time. Monitoring would enable 
us to detect and address issues such as performance degradation, unexpected errors, or increased load promptly. Additionally, 
it would provide valuable insights into usage patterns and system behavior, facilitating informed decisions for scaling and 
optimization. By ensuring the model remains reliable and efficient, monitoring would help maintain user trust and the overall 
effectiveness of the application over time.

## Overall discussion of project

> In the following section we would like you to think about the general structure of your project.

### Question 27

> **How many credits did you end up using during the project and what service was most expensive? In general what do**
> **you think about working in the cloud?**
>
> Recommended answer length: 100-200 words.
>
> Example:
> *Group member 1 used ..., Group member 2 used ..., in total ... credits was spend during development. The service*
> *costing the most was ... due to ... . Working in the cloud was ...*
>
> Answer:

We used a total of 71 kroner in credits during the project. The most expensive service was Google Cloud Storage, which accounted 
for the majority of our expenses. In general, we find working in the cloud to be highly advantageous. The cloud offers great 
scalability and flexibility, making it easy for us to adjust resources as needed and efficiently handle the project's requirements. 
Additionally, the various services and tools available in the cloud have made it easier to collaborate and optimize our workflows.

### Question 28

> **Did you implement anything extra in your project that is not covered by other questions? Maybe you implemented**
> **a frontend for your API, use extra version control features, a drift detection service, a kubernetes cluster etc.**
> **If yes, explain what you did and why.**
>
> Recommended answer length: 0-200 words.
>
> Example:
> *We implemented a frontend for our API. We did this because we wanted to show the user ... . The frontend was*
> *implemented using ...*
>
> Answer:

We have not developed any additional features for our project. However, if we had more time, we would have chosen to implement a 
front-end application for our backend model in relation to the API. This would ensure a more user-friendly interface for users when 
interacting with the project. Ideally, the front-end application would have been developed using the Streamlit framework.

### Question 29

> **Include a figure that describes the overall architecture of your system and what services that you make use of.**
> **You can take inspiration from [this figure](figures/overview.png). Additionally, in your own words, explain the**
> **overall steps in figure.**
>
> Recommended answer length: 200-400 words
>
> Example:
>
> *The starting point of the diagram is our local setup, where we integrated ... and ... and ... into our code.*
> *Whenever we commit code and push to GitHub, it auto triggers ... and ... . From there the diagram shows ...*
>
> Answer:

![SS of our pipeline](figures/q29.jpeg)

Our pipeline operates as follows. A local machine connected to the internet sends a prompt to our cloud-based project. 
The project then uses Docker to set up a temporary virtual machine that acts as an operating system environment. 
This virtual machine makes an API call to our main application file. Depending on the type of request received from the 
local machine, the main file executes the corresponding tasks by accessing the necessary project files. After processing, 
the system returns a value to the local machine based on the original request. This workflow ensures that computationally 
intensive tasks are handled in the cloud, optimizing performance and resource utilization on the local machine. 
By leveraging Docker for managing virtual environments and using API-driven interactions, our pipeline maintains consistency, 
scalability, and efficient communication between local and cloud components.

### Question 30

> **Discuss the overall struggles of the project. Where did you spend most time and what did you do to overcome these**
> **challenges?**
>
> Recommended answer length: 200-400 words.
>
> Example:
> *The biggest challenges in the project was using ... tool to do ... . The reason for this was ...*
>
> Answer:

We dedicated a significant amount of time to setting up our cloud infrastructure. One of the most challenging aspects was 
configuring the Docker images and virtual machines (VMs) to create a robust simulation of an operating system capable of running 
our project seamlessly. This process required meticulous attention to detail to ensure that all components were compatible 
and integrated smoothly.
The primary difficulty lay in coordinating the numerous elements that needed to work together harmoniously. 
Each part of the system, from the Docker containers to the virtual environments, had to be precisely configured to avoid conflicts 
and ensure optimal performance. Additionally, establishing effective debugging methods was crucial. We needed to develop strategies 
to accurately identify and classify issues that arose within different files and components of the project.
In summary, the cloud setup phase was both time-consuming and complex, involving the integration of multiple technologies and the 
development of sophisticated debugging techniques.

### Question 31

> **State the individual contributions of each team member. This is required information from DTU, because we need to**
> **make sure all members contributed actively to the project. Additionally, state if/how you have used generative AI**
> **tools in your project.**
>
> Recommended answer length: 50-300 words.
>
> Example:
> *Student sXXXXXX was in charge of developing of setting up the initial cookie cutter project and developing of the*
> *docker containers for training our applications.*
> *Student sXXXXXX was in charge of training our models in the cloud and deploying them afterwards.*
> *All members contributed to code by...*
> *We have used ChatGPT to help debug our code. Additionally, we used GitHub Copilot to help write some of our code.*
> Answer:

Student s224176 was responsible for the workflow, Student s225785 for the API, and Student s224229 for the cloud infrastructure, job automation, and docker images . 
The remaining tasks were carried out collaboratively, with contributions evenly distributed among all team members. 
Minor usage of ChatGPT, usage of pytorch enviroment with documentation from torch webside and stackoverflow.